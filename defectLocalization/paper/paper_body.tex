\section{Introduction}
Bug localization is one of the important steps in software maintenance.
Especially in issue triaging, where triager has to predict the potential defected components based on his understanding to the issue and the system, then he needs to assign the issue to the right developer who has the required knowledge to fix it.
Automating this step helps speed up bugs fixing and reduce costs in issue triaging.
Lam \etal\cite{lam2017bug} proposed the state of the art approach to automate bug localization by ensemble revised Vector Space Model(rVSM)\cite{zhou2012should} and Deep Neural Network(DNN).  
Similar as most of the existed algorithms, this approach require source code information which is not always available to triagers or the QA team in large companies with code security requirement.
Analyzing the code base dramatically increase the learning time of this kind of algorithm in large projects.
Moreover, the fast evolution of source code further penalizes this approach that the predictive model has to be updated frequently.   
In this paper, we explore the possibility to predict defected components solely based on information learned from bug reports.

Recurrent neural network(RNN) has been proved to be efficient in capturing semantic as well as syntactic information from text to build statistical language models.
But for source code, Hellendoorn \etal show that the RNN is not better than traditional IR method in language model building\cite{hellendoorn2017deep}.
Bug report is different from common text and source code in that it is generally composed by common text and source code segment or error trace message. 
We believe that IR method and RNN can learn different information from bug reports and could complement each other.
Thus the ensemble of these two could offer better performance than using only one of them.

The contribution of this paper is in two folds:  
\begin{itemize}
	\item an ensemble of naive tf-idf based VSM and Gated Recurrent Network to recommend high risk components solely based on bug reports.
	\item By analyzing the performence of the naive tf-idf based VSM and Gated Recurrent Network on their own, and the ensemble of these two, we show that IR method and RNN can learn different information from bug reports and could complement each other to offer better performance in the ensemble method.
\end{itemize}

\section{Background}
\subsection{Bug Localization In Post-release Software Quality Insurrance}
One of the main purpose of software engineering is to deliver high quality product to cients.
However, as software architecture getting more and more complex, the possibility of introducing bugs in the development being higher and higher.  
Different approaches are adopted in the industry to reduce the risk of delivering defeted product, test driven development, unit testing, test regression, peer review etc.
But there are still bugs found by the client after release. 
In this case, being able to fastly locate the defeted components and hence assign the rigth developer to fix the customer reported bugs is critical to post-release software quality insurrance. 

The traditional way is to do the assigning based on the triager's experience and his understanding of the reported issue. 
For experienced triager, they could spot light some potential defected location very fast and accurate.
But for triager with less experience or even new to the project, the process could take longer time and the prediction would not be correct.
Thus they may assign a wrong developer for the issue and reassigning would be required and a longer time to get the bug fixed. 
Jeon \etal\\cite{jeong2009improving} reported that the Eclipse project takes on average 40 days for bug assignment and another 100 days for reassigning..
The numbers for Mozilla are 180 days for the first assigning and another 250 days for reassigning.

To help address this problem, recent research applies NLP techniques to learn from bug reports and suggest a ranked list of potential defect location\cite{gay2009use}\cite{lam2017bug}\cite{nguyen2011topic}\cite{zhou2012should}\cite{saha2013improving}\cite{shokripour2013so}, or to suggest the developer who could fix the bug\cite{anvik2011reducing}\cite{bhattacharya2010fine}\cite{xuan2012developer}\cite{jonsson2016automated}.
All these approaches focus on combining source code and code change history from version controling tools(ex: Git)  with bug reports to make the prediction.
However, in companies that has a strict source code security policy, which is a common situation for large companies, source code or even code change history is not available for triagers. 
Bug report is the only source of information they can use to perform the triaging.   
Therefore, in this work we focus on bug localization with only bug reports as the input source.


\subsection{Data Mining Bug Reports}
The most popular technique used in information retrieval from bug reports is the VSM with TF-IDF as entry value\cite{gay2009use}\cite{zhou2012should}. 
Another frequent used algorithm is the Latent Dirichlet Allocation(LDA)\cite{nguyen2011topic}.
And resent works try to ensemble IR techniques with Machine Learning to achieve better performance\cite{jonsson2016automated}\cite{lam2017bug}.
Jonsson \etal \cite{jonsson2016automated} ensemble TF-IDF with different ML algorithms with Stacked Generalization. 
Lam \etal\ \cite{lam2017bug}combines rVSM with DNN.

To the best of our knowledge, there is not past work to use DNN, more specifically deep Recurrent Neural Network independently to learn from bug reports and suggest potential defect location.
In this work, we apply DNN with multiple layers of Gated Recurrent Unit to learning from bug reports, and compare its performance to VSM.
Moreover, we ensemble the DNN with VSM to leverage the performance.

\section{Methodology}

\subsection{Vector Space Model}
Vector Space Model was first proposed by Salton \etal\cite{salton1975vector} to quantify documents.
With a pre-defined and indexed vocabulary of size $s$: $V = \{t_1, t_2,...,t_s\}$, 
each term $t_i$ in a given document $d$ could be represented by a weight quantity $w_i$, where $i$ is its index in the vocabulary $V$.
Thus the given document $d$ could be represented by a vector $d=[w_1,w_2,...,w_s]$.
Obviously, the length of the vector $d$ is the same as the size of the vocabulary.
If a term $t_j$ is not in document $d$, then $w_j = 0$.   
With this vector representation, we can analyze the documents with quantitative methods.

\subsection{TF-IDF}
The selection of weighting method to use in VSM is important to the performance. 
We choose TF-IDF\cite{salton1988term} because of its well reported performance in previous works.
TF is short for term frequency. 
There are different schemes to evaluate TF, in our implementation, we calcuate TF by fomular 1

\begin{equation}
	tf(t,d) = f_{t,d}
\end{equation}

where $t$ is the given term, $d$ is the document and $f_{t,d}$ is the count of term $t$ in document $d$.

IDF is short for Inverse Document Frequency and is generated from equation 2in our implementation.

\begin{equation}
	idf(t) = log\frac{n_d}{df(d,t)} +1
\end{equation}
where $n_d$ is the total of documents and $df(d,t)$ is the number of documents containing term $t$.

Then the weight $w_t$ is given by:
\begin{equation}
	w_t = tf(t)*idf(t)
\end{equation}
 
Then the resulting vector $w$ is normalized to get the vector $d$:
\begin{equation}
	d = \frac{w}{||w||_2} = \frac{w}{\sqrt{\sum_{i=1}^{s}w_i^2}}
\end{equation} 


\subsection{Our VSM Bug Localization Model}
Similar as the how to rank risky files based on similar bugs in rVSM\cite{zhou2012should},
after we obtain the vector representation of all bug reports, for a new comming report $d_n$ and an old bug report $d_i$ that has been fixed in the training set $D$ , we calculate their Cosine similarity:
\begin{equation}
	similarity(d_n,d_i) = \frac{d_n \cdot d_i}{||d_n||*||d_i||}
\end{equation}

For a given component $c$, define $D_c$ to be the set of bugs that are fixed by changing code in $c$,  then the risk of $c$ is given by:
\begin{equation}
	risk(c) = min(similarity(d_n,d_i)) \forall d_i \in D_c
\end{equation}

And the components are ranked by their risk score from maximum to minimum.
\todo{report performance briefly}





\subsection{Deep Gated Recurrent Unit Network}
Recurrent Neural Network(RNN) is good at modeling sequential data. 
As Mikolov introduced RNN to language modeling\cite{mikolov2010recurrent}, more and more applications of RNN in document analysis related task are proposed, for example, machine translation. 
In recent years, people starts to apply RNN to model source code\cite{raychev2014code}. 
It is believed that RNN can learn syntactic and semantic rules from text documents.
Given that bug reports are also written by human natural language, we believe that we can use RNN to learn from bug reports and make precise predictions on risky components.
We apply Gated Recurrent Unit in our RNN implementation.
GRU is proposed to solve the gradient vanishing problem in training a (RNN)\cite{chung2014empirical}.
It acheive the same performance as Long Short Term Memory but with less computations.
Its stucture is show in figure \ref{gru}. $h$ is the current state and $\hat{h}$ is context state which could memorize the impact of the input history. 
The reset gate $r$ controls whether the state from the current input should be memorized in $\hat{h}$.
The update gate $z$ controls whether the memory from $\hat{h}$ should considered when updating $h$ to give out prediction.  
\begin{figure}
	\includegraphics[scale=0.5]{gru.png}
	\caption{Gated Recurrent Unit\cite{chung2014empirical}}
	\label{gru}
\end{figure}

Given the input vector $x_t$ at step t, each gates and states are calcuated recursively as:
\begin{equation}
	z_t = \sigma_g(W_zx_t+U_{z}h_{t-1}+b_z)
\end{equation}	
\begin{equation}
	r_t =  \sigma_g(W_rx_t+U_{r}h_{t-1}+b_r)
\end{equation}	
\begin{equation}
	h_t=(1-z_t) \circ h_{t-1} + z_t \circ \sigma_h(W_hx_t+U_{h}(r_t \circ h_{t-1})+b_h)
\end{equation}
Where $o$ is point-wise multiplication, $W$ and $U$ is the weight matrices for the corresponding component, $b$ is the bias vector.
$\sigma_g$ is the sigmoid activation function which has the form:
\begin{equation}
	\sigma_g(x) = \frac{1}{1+e^{-1}}
\end{equation}
and $\sigma_h$ is the hyperbolic tangent activation function with the form:
\begin{equation}
\sigma_h(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}}	
\end{equation}

In our DRNN implementation, the input layer pass the input vector to the embedding layer which generates the embedding of size $100$.
The resulted embedding will then be passed to the stacked hidden GRU layers of size $10$.
The encoding of the GRU hidden layers is pass to fit a logistic regression to predict the possibility of each components to be defected.
\todo{draw the graph to show the deep structure}


The number of layers and the embedding size is selected randomly. 
Increasing the number of layers could improve performance a litter with longer training time as penalty.
Small change to the embedding size does not change the result siginicantly
Large change (ex: doubling the embedding size) would result in huge memory requirement and failed in runtime.
\todo {report performance }


\subsection{Ensemble of VSM and GRU}
Ensemble method \cite{dietterich2000ensemble} is used to combine predictions from multiple different model to give out better performance than a single model.	
\cite{lam2017bug} ,\cite{wang2014version} and \cite{jonsson2016automated} apply ensemble method to perform bug triage and achieve good performance. 
Different schemes can be used in ensembling. 
Stacked Generalization\cite{wolpert1992stacked} is theorectally the most effective one which adds one more classifier to take predictions from ensembled models as input and is trained to reduce prediction error.  
Jonsson \etal \cite{jonsson2016automated} ensembled TF-IDF with different ML algorithms with Stacked Generalization.
However, as a preliminary research, we apply a simlper scheme 
For a given component, we takes the minimum of the two risks given by the two aforemention models as the final risk.

\begin{equation}
risk(c) = min(risk_{VSM}(c) , risk_{DNN}(c) )
\end{equation}

\todo {report performace}


 
\section{Experiment Design}

\subsection{Data Set }
We perform our experiment on the exact same data set used in \cite{wang2014version} ,\cite{saha2013improving} and \cite{zhou2012should}.
The detail of the data set is summarized in table \ref{data_table} 


There are in total 3,379 bug reports from four popular open source projects in this data set, AspectJ, Eclipse, SWT, and ZXing. 
\begin{itemize}
	\item ZXing, a barcode image processing library for Android platform, is the smallest data set.
	It contains 20 bugs fixed between March and September 2010 with 391 source files being changed.
	\item SWT is an open source widget tookit. 
	This data set has 98 bugs fixed from October 2004 to April 2010 with 484 source files being fixed. 
	\item AspecJ, An aspect-oriented extension to the Java programming language 
	\item Eclipse, the famous open source integrated development environment, contains 3075 bugs fixed in the period between October 2004 and March 2011 with 12863 files being changed.
\end{itemize}





For each bug report, we can obtain its title, description from the issue reporter, and the files being changed to fixed the reported bug. 

Differently from previous work, the granularity of our experiment is on component level instead of on file level. 
Because we don't use the source code information to simulate a situation of strict source code security, new added files cannot be captured in the prediction with either VSM or DNN.
Thus we first truncate the path of a given file by removing the file name to obtain the component containing the given file. 
We assume that by knowing the potential defected component is enough for assigning the appropriate developer to solve the issue.
Because if a developer knows a component very well, he should also has knowledge to the new added file to that component.
After the truncation, there are\todo{add number} , , , , components in ZXing , SWT, Eclipse and AspectJ respectively. 
We removed one bug from the SWT project since the defected file has no component level, it locates in the top most directory. 
Removing one record does not impact on the performance evaluation given the relatively large number of bugs. 
The detail of the three data sets is summarized in table \ref{data_table}

\subsection{Preprocessing}
Bug reports is different from normal text in that it could has code elements, source file names or error log in its content. 
Here we list some examples to illustrate the situation. 
This is some bug description from the Eclipse project:

\textbf{Bug 77046:}If the Pattern Match job is already running when the console's partitioner reports that it is finished work, it is possible that the pattern matcher can think that it's finished before checking the entire document. This is the trace from the failed test: Incorrect number of messages logged for build. Should be 8. Was 2 junit.framework.AssertionFailedError: Incorrect number of messages logged for build. Should be 8. Was 2 at\\ org.eclipse.ant.tests.ui.BuildTests.testOutput(BuildTests.java:41) at\\ sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at\\ sun.reflect.NativeMethodAccessorImpl.invoke\\(NativeMethodAccessorImpl.java:39) at\\ sun.reflect.DelegatingMethodAccessorImpl\\.invoke(DelegatingMethodAccessorImpl.java:25) at\\ org.eclipse.ant.tests.ui.AbstractAntUIBuildTest.access\$0\\(AbstractAntUIBuildTest.java:1) at\\ org.eclipse.ant.tests.ui.AbstractAntUIBuildTest\$1\\.run(AbstractAntUIBuildTest.java:44) at\\ java.lang.Thread.run(Thread.java:534)



After the normal text describing the issue, the reporter also attached the error trace log as well.
If we don't perform the preprocessing correctly, neither VSM or DRNN model could learn patterns from the error trace log, since the long component name would be seen as one term, but actually there are multiple important terms there. 
For example, the path\\ "org.eclipse.ant.tests.ui.BuildTests.testOutput(BuildTests.java:41)"\\ contains important terms: ui,Build,Test. Here is another example:

\textbf{Bug 76208}
Boolean.getBoolean((String)newValue) queries if a system property exists with the argument name...not what we want in this context.


The reporter is reporting an undesired functionality from a code segment. Again important terms would be missed if we don't separate the code segment correctly. 

Thus we replace all non word character by white space to separate terms, and if a small case character followed by an upper case character, the separate them by a white space. 
We also remove all numbers to reduce the size of the vocabulary and hence the requirement on computation resource. 
After the preprocessing, we summarize the data set detail in table \ref{data_table}.We apply the 80-20\% split to generate the training and testing set for each project.

\begin{table*}[!tbh]
	\begin{center}
		\caption{Data Set Summary}
		\label{data_table}
		
		\begin{tabular}{c|l|l|l|l|l} %
			\textbf{Project} & \textbf{Description} & \textbf{\#bugs} & \textbf{period} & \textbf{\#files} & \textbf{\#component}\\
			\hline\hline
			ZXing & barcode image processing  & 20 & 2010.3-& 391 &11\\
                  &library  for Android platform & &  2010.10 & & \\			
			SWT & open source widget & 98 & 2004.10-& 484 &15\\
			v3.1 &  tookit & &2010.4 & &\\
			AspecJ & An aspect-oriented  & 286 &2002.6 & 6485 & 534\\
			       &   extension to the Java & & -2006.10 & &\\
			       &    programming language & & & &\\
			Eclipse & open source integrated& 3075&2004.10- & 12863& 555\\
 			 v3.1 &  development environment& &2011.3 &  &\\
		\end{tabular}
	\end{center}
\end{table*}

\subsection{Metrics}
To evaluate the performance of different models, we apply the following common used metrics:

\textbf{Top-k Bugs Hit}
Top-k bugs hit is the number of bugs that are correctly localized from the top k components in the output ranked list. 
At least one component has to been captured in the top k list for the given prediction be classified as a hit.

\textbf{Mean Reciprocal Rank(MRR)} is a measure to evaluate the quality of the ranked list from a recommendation system. 
If for all queries under test, the higer the the first hit relevant item is ranked, the higher value the MRR will be and vice versus. 
It's calculated by the following formula:
\begin{equation}
MRR = \frac{1}{|Q|}\sum^{i \in Q}\frac{1}{rank_i}
\end{equation} 
Where $Q$ is the set of test queries, in our context would be the bugs in the testing data set. 
$i$ is a particular bug in $Q$ and $rank_i$ is the ranking of the first collect prediction in the output list.

MRR only cares about the ranking of the first correct prediction. 
However, there could be multiple defected components for one reported bug. 
We also need to check how well different models rank the other defected components.

\textbf{Mean Average Precision(MAP)} To evaluate how well different models rank all defected components we first calculate the average precision for one bug report:
 \begin{equation}
 	AP=\frac{1}{|C|} \sum_{i=1}^{N}prec(i)*relevance(i)
 \end{equation}
Where $C$ is the set of all defected components for the given bug. 
$N$ is the total number of components in the system.  
$prec(i)$ is the precision for the top-i list.
$relevance(i)$ is 1 if the $i^{th}$ item is defected, otherwise $0$.

Then MAP is calculated by the equation:
\begin{equation}
	MAP = \frac{1}{|Q|}\sum^{Q_i \in Q}AP(Q_i)
\end{equation} 

The model with higher MAP score is the better.
\section{Evaluation and Discussion}

\subsection{Experiment Result}

We report the performance of the three models in table \ref{performance_top_k} .
For the project Zxing and SWT, because the their total of components is close to 10, we report the top-1 bug hit rate.
For the project AspectJ and Eclipse, given the their large  number of components, we report the top-10 bug hit rate.

 \begin{table}[h]
 	\begin{center}
 		\caption{Top k bugs hit}
 		\label{performance_top_k}
 		
 		\begin{tabular}{c|l|l|l} %
 			 \textbf{Projects} & \textbf{VSM} & \textbf{DRNN} & \textbf{Ensemble} \\
 			\hline\hline
 			ZXing(top-1)& 4(100\%)&4(100\%) &4(100\%) \\
 			
 			SWT(top-1) & 10(50\%) & 20(100\%) &18(90\%)\\
 			
 			AspecJ(top-10) & 24(41.38\%)  & 46(79.31\%) & 28(48.28)\%\\
 			
 			Eclipse(top-10) &22(3.6\%) & 124(20.29\%)& 26(4.26\%)\\
 			
 		\end{tabular}
 	\end{center}
 \end{table}

 \begin{table}[h]
 	\begin{center}
 		\caption{Mean Reciprocal Rank(MRR)}
 		\label{performance_MRR}
 		
 		\begin{tabular}{c|l|l|l} %
 			\textbf{Projects} & \textbf{VSM} & \textbf{DRNN} & \textbf{Ensemble} \\
 			\hline\hline
 			ZXing& 43.12\%&34.16\% &52.62\% \\
 			
 			SWT & 19.45\% & 67.92\% & 58.63\% \\
 			
 			AspecJ &14.40\%  & 44.10 \% & 46.22\%\\
 			
 			Eclipse &2.38\% & 11.81\%& 4.12\% \\
 			
 		\end{tabular}
 	\end{center}
 \end{table}

 \begin{table}[h]
 	\begin{center}
 		\caption{Mean Average Precision(MAP)}
 		\label{performance_map}
 		
 		\begin{tabular}{c|l|l|l} %
 			\textbf{Projects} & \textbf{VSM} & \textbf{DRNN} & \textbf{Ensemble} \\
 			\hline\hline
 			ZXing& 39.27\%&30.83\% &51.77\%\\
 			
 			SWT & 10.73\%  & 51.13\%  & 39.71\% \\
 			
 			AspecJ &5.74\% & 21.39\% & 15.69\%\\
 			
 			Eclipse &1.18\% & 6.53\%& 1.97\% \\
 			
 		\end{tabular}
 	\end{center}
 \end{table}


\subsection{Is RNN Better Than IR Methods in Mining Bug Report ?}

%\subsection{Is Ensembling Really Improve Performance}








	 